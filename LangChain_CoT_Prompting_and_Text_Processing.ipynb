{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "oBl1D-pEjs6I",
   "metadata": {
    "id": "oBl1D-pEjs6I"
   },
   "source": [
    "# __Demo: Chain of Thought Prompting with LangChain and OpenAI__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lvZdkb4YpOHd",
   "metadata": {
    "id": "lvZdkb4YpOHd"
   },
   "source": [
    "## __Steps to Perform:__\n",
    "Step 1: Set up the OpenAI API Key\n",
    "\n",
    "Step 2: Define a Function to Get Completion\n",
    "\n",
    "Step 3: Define Your Prompts\n",
    "\n",
    "Step 4: Analyze the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q91iZEcUqMci",
   "metadata": {
    "id": "Q91iZEcUqMci"
   },
   "source": [
    "### __Step 1: Set up the OpenAI API Key__\n",
    "- Import the required libraries and set up the OpenAI API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b77ac5df-7119-4777-b0ba-910e5811a852",
   "metadata": {
    "id": "b77ac5df-7119-4777-b0ba-910e5811a852"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "#from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "#_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#openai.api_key  = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8CLt67fQq7FE",
   "metadata": {
    "id": "8CLt67fQq7FE"
   },
   "source": [
    "### __Step 2: Define a Function to Get Completion__\n",
    "- Construct a message with the user's prompt.\n",
    "- Call the __openai.ChatCompletion.create__ method to get a response from the model.\n",
    "- The temperature parameter is set to __0__ for deterministic (non-random) responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "504caddf-3cec-4c96-983f-a73c29bf4223",
   "metadata": {
    "id": "504caddf-3cec-4c96-983f-a73c29bf4223"
   },
   "outputs": [],
   "source": [
    "# def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "#     messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=model,\n",
    "#         messages=messages,\n",
    "#         temperature=0,\n",
    "#     )\n",
    "#     return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CZHiudNysyw4",
   "metadata": {
    "id": "CZHiudNysyw4"
   },
   "source": [
    "### __Step 3: Define Your Prompts__\n",
    "- Provide a series of prompts that guide the model through a chain of thought.\n",
    "- Call the __get_completion__ to get a response from the AI model.\n",
    "- Print both the prompt and the AI-generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "417cc372-7a86-48b6-9400-366a4a7e7992",
   "metadata": {
    "id": "417cc372-7a86-48b6-9400-366a4a7e7992",
    "outputId": "854fd4aa-ca28-4c32-9bec-bcba311693d8"
   },
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     \"Imagine you are a detective trying to solve a mystery.\",\n",
    "#     \"You arrive at the crime scene and start looking for clues.\",\n",
    "#     \"You find a strange object at the crime scene. What is it?\",\n",
    "#     \"How does this object relate to the crime?\",\n",
    "#     \"Who do you think is the suspect and why?\"\n",
    "# ]\n",
    "\n",
    "# for prompt in prompts:\n",
    "#     response = get_completion(prompt)\n",
    "#     print(f\"Prompt: {prompt}\")\n",
    "#     print(f\"Response: {response}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a60f84-00bd-4aa4-af51-a6c1a63a55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de5432eb-f076-4dc3-8b00-36cba1de900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: Introduction: This is a comprehensive guide to splitting and chunking text data for large language\n",
      "Chunk 2: for large language models.\n",
      "Chunk 3: We will explore various techniques and strategies that help in managing long documents.\n",
      "Chunk 4: Section 1: Understanding Tokenization\n",
      "Chunk 5: Tokenization is the process of breaking down text into smaller pieces called tokens.\n",
      "Chunk 6: These tokens could be words, characters, or subwords.\n",
      "Chunk 7: Section 2: The Need for Splitting\n",
      "Chunk 8: LLMs have limitations on the number of tokens they can process at once.\n",
      "Chunk 9: Splitting ensures that the input text adheres to these constraints while maintaining context.\n",
      "Chunk 10: Conclusion: Effective chunking and splitting are essential for optimizing the performance of LLMs.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Example of a long document\n",
    "document = \"\"\"\n",
    "Introduction: This is a comprehensive guide to splitting and chunking text data for large language models.\n",
    "We will explore various techniques and strategies that help in managing long documents.\n",
    " \n",
    "Section 1: Understanding Tokenization\n",
    "Tokenization is the process of breaking down text into smaller pieces called tokens. \n",
    "These tokens could be words, characters, or subwords.\n",
    "\n",
    "Section 2: The Need for Splitting\n",
    "LLMs have limitations on the number of tokens they can process at once. \n",
    "Splitting ensures that the input text adheres to these constraints while maintaining context.\n",
    "\n",
    "Conclusion: Effective chunking and splitting are essential for optimizing the performance of LLMs.\n",
    "\"\"\"\n",
    "\n",
    "# Use a recursive character splitter to handle complex text\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "\n",
    "# Split the document\n",
    "chunks = splitter.split_text(document)\n",
    "\n",
    "# Display the chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a4497a5-fa85-4711-96f8-a3578a52b96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: Introduction: This is a comprehensive guide to splitting and chunking text data for large language\n",
      "Embedding 1 (first 10 dimensions): [0.002186535500513491, 0.026265843209877716, 0.018259969433441076, -0.02005580884968156, 0.009342469675332582, 0.02226290725298192, -0.019027656725419263, -0.008821540010764371, -0.021851646403277, -0.0331201869796696]\n",
      "\n",
      "Chunk 2: for large language models.\n",
      "Embedding 2 (first 10 dimensions): [-0.018505490438297193, 0.007317708458003559, 0.0036486338955528685, -0.013524814798472376, 0.015262259377594335, 0.011460321411630333, -0.012911599448150375, 0.013701966313597635, 0.00200146823007106, -0.03510319647533471]\n",
      "\n",
      "Chunk 3: We will explore various techniques and strategies that help in managing long documents.\n",
      "Embedding 3 (first 10 dimensions): [-0.007153482990749162, 0.017582218806624198, 0.024982373753098856, -0.021145257753182455, 0.003960453374762989, 0.020898585797456955, -0.024763110620296243, 0.014772902001348991, -0.01981596988829255, -9.207368415910364e-05]\n",
      "\n",
      "Chunk 4: Section 1: Understanding Tokenization\n",
      "Embedding 4 (first 10 dimensions): [-0.006697608936889314, 0.00696292784964458, -0.0018419274990411334, -0.02291269335021089, -0.006959526659443017, -0.007680650228354779, -0.030994724069222747, 0.014749025684457939, -0.011347497412783993, -0.03172945379592469]\n",
      "\n",
      "Chunk 5: Tokenization is the process of breaking down text into smaller pieces called tokens.\n",
      "Embedding 5 (first 10 dimensions): [-0.020924610689651693, 0.016903554756160552, -0.004043113842664431, -0.013260656760238656, 0.0006452280112151149, 0.006850919113384215, -0.029596978335335578, -0.0078593342266389, -0.00967448096483605, -0.03400879300063332]\n",
      "\n",
      "Chunk 6: These tokens could be words, characters, or subwords.\n",
      "Embedding 6 (first 10 dimensions): [-0.008876602332272408, -0.0034838994688573626, -0.003221939611282012, -0.011492863513517035, -0.023399524830585616, 0.0042814589910418815, -0.027203782041665486, -0.017526284189683574, -0.0012922797129350213, -0.02050294951895072]\n",
      "\n",
      "Chunk 7: Section 2: The Need for Splitting\n",
      "Embedding 7 (first 10 dimensions): [0.0030144650994463843, 0.017938293540266683, 0.009761124862178155, -0.02176618578917146, -0.01808349045513081, 0.02031422781640112, -0.016380738488557034, 0.012849837558507774, -0.007642584397280855, -0.009371736099250743]\n",
      "\n",
      "Chunk 8: LLMs have limitations on the number of tokens they can process at once.\n",
      "Embedding 8 (first 10 dimensions): [-0.014162577839487336, 0.006090993992715791, -0.003362594894944597, -0.03098403154123878, -0.03215068079314792, 0.0013616558739093243, -0.04357298796509838, 0.0009368803136634344, -0.022003546175728163, -0.03413126878455839]\n",
      "\n",
      "Chunk 9: Splitting ensures that the input text adheres to these constraints while maintaining context.\n",
      "Embedding 9 (first 10 dimensions): [-0.0006993512714826328, 0.011440446009630968, 0.01654057253972379, -0.0057933807235362135, 0.004995614904641535, 0.021292329468505927, -0.02113904680667632, -0.003511562116691611, -0.026058020846064355, -0.0346418368700058]\n",
      "\n",
      "Chunk 10: Conclusion: Effective chunking and splitting are essential for optimizing the performance of LLMs.\n",
      "Embedding 10 (first 10 dimensions): [-0.002297317888888947, 0.029037286188519175, 0.006588917825680459, -0.05160077394017079, -0.016631430354264138, 0.018202478437433788, -0.016143864350054656, -0.007814605879766233, -0.01963808986374953, -0.04886498774439322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Step 2: Initialize the OpenAI Embeddings model\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Generate embeddings for each chunk using the correct method\n",
    "chunk_embeddings = embedding_model.embed_documents(chunks)\n",
    "\n",
    "# Step 3: Display the embeddings for each chunk\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "    print(f\"Embedding {i+1} (first 10 dimensions): {embedding[:10]}\")\n",
    "    print()  # Print a new line for better readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b105c50-25f0-4f0a-b10f-c139ac2167d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
